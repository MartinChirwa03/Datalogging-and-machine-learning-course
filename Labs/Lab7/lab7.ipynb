{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7\n",
    "In this lab we will be working with classification and clustering models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn \n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get your Data and Analyse\n",
    "\n",
    "For this lab, we use the Titanic Dataset to train different types of classification models and analyze their differences, effectiveness, and decision-making processes. \n",
    "This dataset includes a set of features - e.g. age, deck,... - inlcuding a label on whether they survived on the titanic. We will use the dataset to train classification models that are trying to find patterns in the features of the data to predic one of the labels - for instance whether they survived or not.\n",
    "\n",
    "First, we need to analyse and preprocess our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "We start by loading and inspecting the dataset to understand its structure, identify missing values, and explore feature distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Titanic Dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# print(titanic.info())\n",
    "# print(titanic.describe(include='all'))\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Seaborn to visualize the distribution of missing data points. This helps in deciding how to handle them before training models.\n",
    "\n",
    "If you find a lot of missing data in a column, think about what to do with it:\n",
    "\n",
    "    - Drop the column if too much data is missing, making it unreliable.\n",
    "    - Fill missing values using mean, median, mode, or another method.\n",
    "    - Drop rows if only a few values are missing and wonâ€™t affect the dataset significantly.\n",
    "\n",
    "Complete the code below to inspect missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "sns.heatmap(..., yticklabels=False, cbar=False, cmap='coolwarm') # complete the code -> visualise the missing values\n",
    "plt.title('Missing Data Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Handle Missing Data:*\n",
    "\n",
    "Look at the heatmap and decide:\n",
    "\n",
    "    - Which columns have too much missing data and should be removed?\n",
    "    \n",
    "    - Which columns should be filled, and with what method (mean, median, mode)? Look into: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html#pandas.DataFrame.interpolate\n",
    "\n",
    "Modify the code below based on your decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns or fill missing values based on your analysis\n",
    "# Example: Drop colomn because it's mostly empty\n",
    "# Example: Fill columns withfew missing values with median/mode\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "titanic = ...#complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll need to convert categorical features to dummy variables using pandas! Otherwise our machine learning algorithm won't be able to directly take in those features as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_preprocessed = pd.get_dummies(titanic, columns=[\"sex\",\"embarked\",\"class\",\"who\",\"deck\",\"embark_town\",\"alive\"], drop_first=True)\n",
    "# what does this do? explain by showing new dataframe!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to evaluate which features in the dataset are relevant for classification. Understanding feature relationships helps in making informed choices about which variables to include in the model.\n",
    "\n",
    "Try out different statistical analyses to explore possible research questions. Look for interesting relationships you could investigate.\n",
    "\n",
    "To do that, you could:\n",
    "\n",
    "    - Plot the covariances between all numerical features in a heatmap.\n",
    "    - Check correlations to see which variables are strongly linked to survival.\n",
    "    - Pick several subgroups (e.g., based on class, age, or gender) to analyze differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as an example, try plotting the correlation matrix between all numerical features to see which ones would be good predictors/most interesting to look at\n",
    "plt.figure(figsize=(12, 8))\n",
    "#sns.heatmap(titanic.corr(numeric_only=True), cmap='coolwarm', annot=True, fmt=\".2f\") # this is only plotting the correlation between numerical features, alternatively you could try converting categorical features to numerical ones to plot all features\n",
    "sns.heatmap(titanic_preprocessed.corr(), cmap='coolwarm', annot=True, fmt=\".2f\") # this is for the converted features into numeric values, explain the\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the heatmap and think about:\n",
    "\n",
    "    - Which features have the strongest correlation with survival?\n",
    "    - Do certain groups (e.g., male vs. female, first-class vs. third-class) show different trends?\n",
    "    - Are there redundant features that might not add value?\n",
    "\n",
    "Based on this, think about whether you want to remove certain features from your classification or combine features to create new, more meaningful variables. Some features might be highly correlated and not add much new information, while others might interact in ways that improve classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've selected the most relevant features, the next step is to prepare your data for modeling, ensuring it's in the right format for classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X1 = titanic_preprocessed[['feature1', 'feature2',...]]  # complete with the features you want to use/ or alternatively use all of your preprocessed ones\n",
    "y = titanic_preprocessed['...']  # ADD target variable (the feature you want to predict)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)\n",
    "# Apply StandardScaler if needed (for SVM, Logistic Regression, K-Means)\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification 1 - SVM\n",
    "At first, we will train and test Support Vector Machine Classifiers on your three preprocessed training datasets and evaluate their performance on predicting the survival rate on the titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Support Vector Classifier for your classification task\n",
    "\n",
    "# train the model\n",
    "clf = ...\n",
    "#clf.fit(x_train, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate your test performance in terms of accuracy - using accuracy_score\n",
    "# Example testing : y_pred = clf.predict(x_test)\n",
    "# accuracy = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Logistic Regression\n",
    "Now, we will train and test Logistic Regression Classifiers on your three preprocessed training datasets and evaluate their performance on predicting the survival rate on the titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Logistic Regression Classifier\n",
    "\n",
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate your test performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Random Forest\n",
    "Now, we will train and test Random Forest Classifiers on your three preprocessed training datasets and evaluate their performance on predicting the survival rate on the titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Random Forest Classifier\n",
    "\n",
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate your test performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse your different model performances\n",
    "\n",
    "As so far, we are dealing with labelled data, we can calculate the performance of the different models to analyse different performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, plot the accuracy performance of each model using a comparative boxplot  \n",
    "# Boxplot for algorithm comparison  \n",
    "fig = plt.figure(figsize=(15,6))  \n",
    "fig.suptitle('Classifier Algorithm Comparison', fontsize=22)  \n",
    "ax = fig.add_subplot(111)  \n",
    "\n",
    "sns.boxplot(x=..., y=...)  # Add data  \n",
    "\n",
    "ax.set_xticklabels(...)  # Add labels  \n",
    "ax.set_xlabel(\"Algorithm\", fontsize=20)  \n",
    "ax.set_ylabel(\"Accuracy of Models\", fontsize=18)  \n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more in-depth analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate and print evaluation metrics for each model\n",
    "# accuracy_model1 = ...\n",
    "# precision_model1 = ...\n",
    "# recall_model1 = ...\n",
    "# f1_model1 = ...\n",
    "# print(...)\n",
    "\n",
    "# Plot confusion matrix for each of the models\n",
    "# cm_model1 = ...\n",
    "# cm_model2 = ...\n",
    "# cm_model3 = ...\n",
    "\n",
    "# Visualize confusion matrix using seaborn heatmap\n",
    "# plt.figure(figsize=(6,4))\n",
    "# sns.heatmap(..., annot=True, fmt='d', cmap='Blues')\n",
    "# plt.title(...)\n",
    "# plt.xlabel(...)\n",
    "# plt.ylabel(...)\n",
    "# plt.show()\n",
    "\n",
    "# Plot ROC curve for each model\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# fpr, tpr, _ = ...\n",
    "# roc_auc = ...\n",
    "# plt.plot(...)\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "# precision, recall, _ = ...\n",
    "# plt.plot(...)\n",
    "\n",
    "# Compare models based on evaluation metrics\n",
    "# print(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this analysis tell you? Which or your trained models are performing best, for which of your specific tasks? Can you guess why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Clustering: K-Means\n",
    "K-Means is a clustering algorithm that groups unlabeled data into clusters based on similarity. It tries to find meaningful patterns in the data without any predefined categories.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "    - Generate or select data for clustering.\n",
    "    - Apply K-Means with different values of k (the number of clusters).\n",
    "    - Analyze the results to understand how the choice of k affects clustering.\n",
    "\n",
    "Since we don't have labels, we don't know the best value of k in advance. You will experiment with different values and evaluate the results.\n",
    "\n",
    "You should use the function *KMeans* in **sklearn.cluster** https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "For your reference, here is a tutorial on k-means and Python:\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate 100 random values from a 2-d multivariate normal distribution. This is a distribution with just one peak - in other words, we will assume all data are from the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbpts1 = 100\n",
    "mu1 = [0, 0]  # the mean of the first cluster (this is a vector as we are in 2D)\n",
    "sigma1 = [[1, 0], [0, 1]]  # the covariance matrix of the first cluster -- this one is not correlated\n",
    "data = np.random.multivariate_normal(mu1, sigma1, nbpts1)\n",
    "plt.scatter(data[:, 0], data[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply k-means with k taking values from 5 to 1. For each k, plot the final clustering to see what has happened. Plot the centroids too. Which value of k is best? Does that make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINT: You can use the following code, where idx is an array containing the class labels\n",
    "# obtained from the k-means algorithm.\n",
    "# This code is used in the tutorial referred to above.\n",
    "for k in np.arange(5, 0, -1):\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(data)  # Fit KMeans model with k cluster\n",
    "    idx = kmeans....  # COMPLETE: Predict class membership\n",
    "    plt.figure(k)\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=idx, s=50, cmap='viridis')\n",
    "    C = kmeans.cluster_centers_\n",
    "    plt.scatter(C[:, 0], C[:, 1], c='black', s=200, alpha=0.5)  # Plotting centroids\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow method helps determine the best number of clusters (k) for K-Means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = ...# add\n",
    "wcss = ...#add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within-Cluster Sum of Squares (WCSS) -> total varience in cluster - sum of the squared distances between each data point and the centroid of the cluster it belongs to\n",
    "# Plot the elbow graph\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, wcss, marker='o', linestyle='--')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.xticks(k_values)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extention Task: Now, apply clustering on either your own data or a datset from Kaggle(e.g. the Iris Dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
